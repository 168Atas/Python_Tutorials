{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_dataset.reshape((-1, 28*28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "V_accur_list = [] #verification accuracy list\n",
    "step_list = [] # step number list\n",
    "cost_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 16.018076\n",
      "Training accuracy: 9.6%\n",
      "Validation accuracy: 11.7%\n",
      "Loss at step 10: 5.673235\n",
      "Training accuracy: 39.2%\n",
      "Validation accuracy: 40.6%\n",
      "Loss at step 20: 4.027567\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 55.3%\n",
      "Loss at step 30: 3.498514\n",
      "Training accuracy: 60.5%\n",
      "Validation accuracy: 61.0%\n",
      "Loss at step 40: 3.195328\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 63.9%\n",
      "Loss at step 50: 2.986209\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 65.8%\n",
      "Loss at step 60: 2.828773\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 67.4%\n",
      "Loss at step 70: 2.703155\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 68.5%\n",
      "Loss at step 80: 2.598658\n",
      "Training accuracy: 69.2%\n",
      "Validation accuracy: 69.4%\n",
      "Loss at step 90: 2.509296\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 70.0%\n",
      "Loss at step 100: 2.431398\n",
      "Training accuracy: 70.6%\n",
      "Validation accuracy: 70.4%\n",
      "Loss at step 110: 2.362425\n",
      "Training accuracy: 71.1%\n",
      "Validation accuracy: 70.9%\n",
      "Loss at step 120: 2.300595\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 71.1%\n",
      "Loss at step 130: 2.244609\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 71.5%\n",
      "Loss at step 140: 2.193487\n",
      "Training accuracy: 72.2%\n",
      "Validation accuracy: 71.7%\n",
      "Loss at step 150: 2.146482\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 71.8%\n",
      "Loss at step 160: 2.102998\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 71.9%\n",
      "Loss at step 170: 2.062561\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 72.2%\n",
      "Loss at step 180: 2.024787\n",
      "Training accuracy: 73.1%\n",
      "Validation accuracy: 72.4%\n",
      "Loss at step 190: 1.989350\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 72.5%\n",
      "Loss at step 200: 1.955979\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 72.6%\n",
      "Loss at step 210: 1.924445\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 72.7%\n",
      "Loss at step 220: 1.894559\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 72.8%\n",
      "Loss at step 230: 1.866155\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 72.9%\n",
      "Loss at step 240: 1.839091\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 73.0%\n",
      "Loss at step 250: 1.813251\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 73.1%\n",
      "Loss at step 260: 1.788532\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 73.2%\n",
      "Loss at step 270: 1.764843\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 73.3%\n",
      "Loss at step 280: 1.742107\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 73.3%\n",
      "Loss at step 290: 1.720259\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 73.4%\n",
      "Loss at step 300: 1.699234\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 73.5%\n",
      "Loss at step 310: 1.678980\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 73.6%\n",
      "Loss at step 320: 1.659446\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 73.6%\n",
      "Loss at step 330: 1.640590\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 73.6%\n",
      "Loss at step 340: 1.622369\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 73.7%\n",
      "Loss at step 350: 1.604745\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 73.7%\n",
      "Loss at step 360: 1.587687\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 73.7%\n",
      "Loss at step 370: 1.571164\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 380: 1.555143\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 73.8%\n",
      "Loss at step 390: 1.539603\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 73.9%\n",
      "Loss at step 400: 1.524515\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 74.0%\n",
      "Loss at step 410: 1.509863\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 74.1%\n",
      "Loss at step 420: 1.495623\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 74.1%\n",
      "Loss at step 430: 1.481774\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 74.1%\n",
      "Loss at step 440: 1.468301\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 74.2%\n",
      "Loss at step 450: 1.455188\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 74.2%\n",
      "Loss at step 460: 1.442418\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 74.3%\n",
      "Loss at step 470: 1.429980\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 74.3%\n",
      "Loss at step 480: 1.417859\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 490: 1.406039\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 500: 1.394512\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 510: 1.383266\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 520: 1.372290\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 530: 1.361574\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 74.4%\n",
      "Loss at step 540: 1.351107\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 550: 1.340881\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 560: 1.330886\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 570: 1.321114\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 580: 1.311558\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 590: 1.302211\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 600: 1.293063\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 74.5%\n",
      "Loss at step 610: 1.284109\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 620: 1.275342\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 630: 1.266756\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 640: 1.258344\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 650: 1.250100\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 74.7%\n",
      "Loss at step 660: 1.242022\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 670: 1.234101\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 680: 1.226331\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 690: 1.218711\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 700: 1.211235\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 710: 1.203897\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 720: 1.196694\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 730: 1.189620\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 740: 1.182675\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 750: 1.175851\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 760: 1.169146\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 770: 1.162557\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 780: 1.156081\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 75.0%\n",
      "Loss at step 790: 1.149713\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 800: 1.143452\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 83.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 10 == 0):\n",
    "            cost_list.append(l)\n",
    "            step_list.append(step)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "            predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            valid_accuracy = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            V_accur_list.append(valid_accuracy)\n",
    "            print('Validation accuracy: %.1f%%' % valid_accuracy)\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA38AAAFUCAYAAACUQ4VTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e71af90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5));\n",
    "axes[0].plot(step_list, cost_list);\n",
    "axes[0].set_title('Cost vs Steps');\n",
    "axes[0].set_xlabel('Steps', size = 12);\n",
    "axes[0].set_ylabel('Cost', size = 12);\n",
    "axes[1].plot(step_list, V_accur_list);\n",
    "axes[1].set_title('Validation Accuracy vs Steps');\n",
    "axes[1].set_xlabel('Steps', size = 12);\n",
    "axes[1].set_ylabel('Validation Accuracy', size = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 18.240400\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 8.5%\n",
      "Minibatch loss at step 500: 2.371244\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 1000: 1.244738\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 1500: 1.324030\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 2000: 1.123285\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 2500: 1.130614\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 3000: 0.932454\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 3500: 0.729505\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 4000: 1.040036\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 4500: 0.717898\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 5000: 0.574863\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.7%\n",
      "Test accuracy: 86.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the graph always comes first\n",
    "\n",
    "batch_size = 128\n",
    "n_hidden_nodes = 1024\n",
    "image_size = 28\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be \n",
    "    # fed at runtime with a training minibatch\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    weights_01 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, n_hidden_nodes]))\n",
    "    weights_12 = tf.Variable(tf.truncated_normal([n_hidden_nodes, num_labels]))\n",
    "    biases_01 = tf.Variable(tf.zeros([n_hidden_nodes]))\n",
    "    biases_12 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    z_01= tf.matmul(tf_train_dataset, weights_01) + biases_01\n",
    "    h1 = tf.nn.relu(z_01)\n",
    "    z_12 = tf.matmul(h1, weights_12) + biases_12\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(z_12, tf_train_labels))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(z_12)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_01) + biases_01), weights_12) + biases_12)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_01) + biases_01), weights_12) + biases_12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 324.309784\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 26.4%\n",
      "Minibatch loss at step 500: 15.666269\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 9.701218\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1500: 8.730391\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 5.601308\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 2500: 9.332880\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 3000: 3.054952\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 3500: 3.030112\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 4000: 2.456403\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 4500: 2.019478\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 5000: 0.738836\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 5500: 2.041015\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 6000: 2.410986\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 6500: 2.461916\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 7000: 1.873383\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 7500: 1.002510\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 8000: 0.986571\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 8500: 0.415871\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 9000: 1.346511\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 9500: 0.744818\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 10000: 1.356037\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 10500: 1.892023\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 11000: 0.966964\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 11500: 0.858190\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 12000: 0.805250\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 12500: 2.565210\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 13000: 0.797943\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 13500: 0.319276\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 14000: 0.922709\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 14500: 1.209756\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 15000: 0.429418\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 15500: 0.444208\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 16000: 0.633546\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 16500: 0.620418\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 17000: 0.509932\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 17500: 0.342126\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 18000: 0.518216\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 18500: 1.201865\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 19000: 0.428867\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 19500: 0.515053\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 20000: 0.336005\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.6%\n",
      "Test accuracy: 92.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            cost2_list.append(l)\n",
    "            step2_list.append(step)\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a71bdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#very simple plotting\n",
    "fig = plt.figure(1)\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.set_title('Cost vs Steps')\n",
    "ax1.plot(step2_list,cost2_list,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
