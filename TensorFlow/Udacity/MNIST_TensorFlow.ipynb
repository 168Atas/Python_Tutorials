{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "<h1 align=\"center\"> MNIST TensorFlow </h1>\n",
    "\n",
    "A large part of this tutorial's purpose is to visualize weights using TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import unpack\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into normalizing the data here. Maybe not instructional to have separate cell on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadmnist(imagefile, labelfile):\n",
    "\n",
    "    # Open the images with gzip in read binary mode\n",
    "    images = open(imagefile, 'rb')\n",
    "    labels = open(labelfile, 'rb')\n",
    "\n",
    "    # Get metadata for images\n",
    "    images.read(4)  # skip the magic_number\n",
    "    number_of_images = images.read(4)\n",
    "    number_of_images = unpack('>I', number_of_images)[0]\n",
    "    rows = images.read(4)\n",
    "    rows = unpack('>I', rows)[0]\n",
    "    cols = images.read(4)\n",
    "    cols = unpack('>I', cols)[0]\n",
    "\n",
    "    # Get metadata for labels\n",
    "    labels.read(4)\n",
    "    N = labels.read(4)\n",
    "    N = unpack('>I', N)[0]\n",
    "\n",
    "    # Get data\n",
    "    x = np.zeros((N, rows*cols), dtype=np.uint8)  # Initialize numpy array\n",
    "    y = np.zeros(N, dtype=np.uint8)  # Initialize numpy array\n",
    "    for i in range(N):\n",
    "        for j in range(rows*cols):\n",
    "            tmp_pixel = images.read(1)  # Just a single byte\n",
    "            tmp_pixel = unpack('>B', tmp_pixel)[0]\n",
    "            x[i][j] = tmp_pixel\n",
    "        tmp_label = labels.read(1)\n",
    "        y[i] = unpack('>B', tmp_label)[0]\n",
    "\n",
    "    images.close()\n",
    "    labels.close()\n",
    "    return (x, y)\n",
    "\n",
    "def non_flattened_image(image):\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def flattened_image(image):\n",
    "    plt.imshow(np.reshape(image, (784,1)), cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img_path = 'train-images-idx3-ubyte' \n",
    "train_lbl_path = 'train-labels-idx1-ubyte' \n",
    "test_img_path = 't10k-images-idx3-ubyte' \n",
    "test_lbl_path = 't10k-labels-idx1-ubyte'\n",
    "\n",
    "x_train, y_train = loadmnist(train_img_path, train_lbl_path)\n",
    "x_train, y_train = np.array(x_train).real.astype(np.float32, copy=False), np.array(y_train).real.astype(np.float32, copy=False) \n",
    "testing_data, testing_labels = loadmnist(test_img_path, test_lbl_path)\n",
    "testing_data, testing_labels = np.array(testing_data).real.astype(np.float32, copy=False), np.array(testing_labels).real.astype(np.float32, copy=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"left\"> Making a Validation Test Set </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training, validation, and test data set explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the dataset is 60000 by 784. Each of the images is shaped flattened\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 45000\n",
      "validation size: 15000\n",
      "training + validation size: 60000\n"
     ]
    }
   ],
   "source": [
    "training_number = len(x_train) * 3/4\n",
    "validation_number = len(x_train) * 1/4\n",
    "total_number = training_number + validation_number\n",
    "print ('training size: {}'.format(training_number))\n",
    "print ('validation size: {}'.format(validation_number))\n",
    "print ('training + validation size: {}'.format(training_number + validation_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = x_train[0:training_number], x_train[training_number:total_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_labels, validation_labels = y_train[0:training_number], y_train[training_number:total_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"left\"> Show example of the data </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> How the Computer Sees the Data With Multinomial Logistic Regression </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Logistic Regression? What is Multinomial Logistic Regression (Softmax Regression)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't account for the interrelationships between pixels as you can see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADEAAAEACAYAAAAazZ6gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAwdJREFUeJztnD8OKXEUhe+MJwollUSEQmELlqC2AbuwASX2MJWYJWgU\nkjFL0DBRSTQKBcLvVa942rnJPW7Ot4DD5zf3nol/UQhBfp3Y+gloQAkUKIECJVCgBAqUQMFMIoqi\nEEWRyo0bTwIFSqBACRQogQIlUKAECpRAgRIoUAIFSqBACRQogQIlUKAECpRAgRIoUAIFSqBACRQo\ngYILiT9WDxzHeq+fmcRgMFDLMpNoNBpqWZHVl3sPh0MQEen3+1HZLDOJf9+wCSGUlnCxnVxImA12\np9NRyzKTmM1mallmg/35fIKISBzHvzvYm81GNpuNSpbZ5XS5XNSy2BMouJAwm4nb7aaWxRWLgtlJ\njMfjICKyXq9Ln4TZTOz3e7Us9gQKLiTMZuJ8PqtlsSfKkGWZZFmmksXthAIlUHAhYdYTk8lELctF\nT5hJ1Ov1ICJyv99/V4I98QUlUHAhYdYTlUpFLctM4nQ6qWVxxaJACRQogYILCfZEqQdmT/wPJVCg\nBAouJMx6YrlcqmWxJ1CgBAqUQMGFhFlPtFottSwziel0qpbFskOBEihQAgUXEmY98X6/1bLMeqJW\nqwURkcfjwQ/jRZzMBCVQoAQKZj2R57laltmKHQ6HQURkt9uxJ0SczAQlUKAECmY90ev11LLMJFar\nlVoWewIFSqBACRRcSJj1xPP5VMtiT6BACRQogYLZil0sFmpZZhKj0Ugtiz2BAiVQcCFhtp3a7bZa\nlosflZtdTnmeq33Qwp5AgRIouJAw64ntdquW5aInzCSazWYQEbler78rwZ74ghIomK3YbrerluVi\nxZpdTmmaSpqmKllcsShQAgUXEvy/2DK4uBXnTw++cDHYlEDBbMXO53O1LBcr1uxySpJEkiRRyeKK\nRYESKLiQMOuJ4/GolmV2EkVRSFEUKllmK7ZarQYRkdfrxVtxESeDTQkUKIECJVCgBAqUQIESKFAC\nBUqgQAkUKIECJVCgBAqUQMHsvVhNXJwEJVCgBAqUQIESKFACBUqgQAkUKIECJVBwIfEXAFxV2//n\nAKwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x157dea2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flattened_image(training_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> How we See the Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12905a250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "non_flattened_image(training_data[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"left\"> What is Logistic Regression? </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "research from tensorflow website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"left\"> What is Multinomial Logistic Regression (Softmax Regression)? </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "research from tensorflow website (maybe the image from the website) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're first going to train a multinomial logistic regression (sigmoid regression) using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.train.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.182138978\n",
      "Epoch: 0002 cost= 0.664755007\n",
      "Epoch: 0003 cost= 0.552493345\n",
      "Epoch: 0004 cost= 0.498611760\n",
      "Epoch: 0005 cost= 0.465482047\n",
      "Epoch: 0006 cost= 0.442566673\n",
      "Epoch: 0007 cost= 0.425448460\n",
      "Epoch: 0008 cost= 0.412102622\n",
      "Epoch: 0009 cost= 0.401347178\n",
      "Epoch: 0010 cost= 0.392331279\n",
      "Epoch: 0011 cost= 0.384745220\n",
      "Epoch: 0012 cost= 0.378130858\n",
      "Epoch: 0013 cost= 0.372387048\n",
      "Epoch: 0014 cost= 0.367285533\n",
      "Epoch: 0015 cost= 0.362673161\n",
      "Epoch: 0016 cost= 0.358586517\n",
      "Epoch: 0017 cost= 0.354888838\n",
      "Epoch: 0018 cost= 0.351474208\n",
      "Epoch: 0019 cost= 0.348237709\n",
      "Epoch: 0020 cost= 0.345426560\n",
      "Epoch: 0021 cost= 0.342725641\n",
      "Epoch: 0022 cost= 0.340232040\n",
      "Epoch: 0023 cost= 0.337920682\n",
      "Epoch: 0024 cost= 0.335737725\n",
      "Epoch: 0025 cost= 0.333696858\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9138\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'labels' of 'SoftmaxCrossEntropyWithLogits' Op has type bool that does not match type float32 of argument 'features'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-553cd9ba2289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# cross-entropy across all training examples: that's our loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mgalarny/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(logits, labels, name)\u001b[0m\n\u001b[1;32m    407\u001b[0m   \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m--> 409\u001b[0;31m       logits, labels, name=name)\n\u001b[0m\u001b[1;32m    410\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mgalarny/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   1335\u001b[0m   \"\"\"\n\u001b[1;32m   1336\u001b[0m   result = _op_def_lib.apply_op(\"SoftmaxCrossEntropyWithLogits\",\n\u001b[0;32m-> 1337\u001b[0;31m                                 features=features, labels=labels, name=name)\n\u001b[0m\u001b[1;32m   1338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_SoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mgalarny/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    467\u001b[0m                   \u001b[0;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 469\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'labels' of 'SoftmaxCrossEntropyWithLogits' Op has type bool that does not match type float32 of argument 'features'."
     ]
    }
   ],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(training_data[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(training_labels[:train_subset], 10)\n",
    "    tf_valid_dataset = tf.constant(validation_data)\n",
    "    tf_test_dataset = tf.constant(testing_data)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "V_accur_list = [] #verification accuracy list\n",
    "step_list = [] # step number list\n",
    "cost_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 20 == 0):\n",
    "            cost_list.append(l)\n",
    "            step_list.append(step)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "            predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            valid_accuracy = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            V_accur_list.append(valid_accuracy)\n",
    "            print('Validation accuracy: %.1f%%' % valid_accuracy)\n",
    "    test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "    print('Test accuracy: %.1f%%' % test_accuracy)\n",
    "    #This line below allows for taking the predictions to a pandas dataframe and eventually a heatmap/confusion matrix\n",
    "    pred = test_prediction.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing The Data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
